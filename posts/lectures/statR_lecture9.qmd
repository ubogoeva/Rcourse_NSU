---
title: "Язык программирования R для анализа данных: лекция 9"
subtitle: 'Ограничения ANOVA, линейная регрессия'
author: "Elena U"
#date: "Created on 01 April, 2023"
execute:
  echo: true
  output: true
format: 
  revealjs:
    slide-number: c/t
    show-slide-number: all
    # mainfont: Arial
    # fontsize: 14px
    theme: [default, custom.scss]
    chalkboard: 
      buttons: true
    # theme: [serif]
    # mouse-wheel: true
    auto-play-media: true
    width: 1280
    height: 720
    fig-dpi: 300
    # logo: figures/icg.png
revealjs-plugins:
  - pointer
editor: visual
draft: true
---

## План лекции

-   ::: {.fragment .fade-in-then-out}
    Ограничения ANOVA
    :::

-   Ковариация и корреляция

-   Простая линейная регрессия: формула, реализация в R, интерпретация

-   ::: {.fragment .highlight-blue}
    Ограничения линейной регрессии (в т.ч. нормальность распределения остатков)
    :::

<!-- -->

-   Множественная линейная регрессия

-   ::: {.fragment .fade-in-then-out}
    Ограничения ANOVA
    :::

# Ковариация и корреляция

## Данные для работы

Датасет с [каггла](https://www.kaggle.com/datasets/levyedgar44/income-and-happiness-correction) о связи ВВП (GDP) на душу населения и счастья.

```{r}
library(tidyverse)
df <- read_csv('https://raw.githubusercontent.com/ubogoeva/Rcourse_NSU/master/posts/lectures/data/happyscore_income.csv')
head(df)
df <- df %>% 
  rename(country = country...1) %>% 
  select(!country...11)
```

## Посмотрим на данные

Используя базовую generic-функцию `summary()`.

```{r}
summary(df)
```

Больше всего нас будут интересовать переменные `happyScore` и `GDP`.

## Посмотрим на данные

С помощью функции `skim()` из пакета `skimr`.

```{r}
#| eval: false
skimr::skim(df)
```

![](images/image-419789197.png)

(Аутпут очень большой, поэтому прикрепила скрином).

## Ковариация

Самая простая мера связи между двумя количественными переменными --- это ковариация. Если ковариация положительная, то чем больше одна переменная, тем больше другая переменная. При отрицательной ковариации наоборот: чем больше одна переменная, тем меньше другая. -\> проиллюстрировать на графике

Формула ковариации:

$$
\sigma_{xy} = cov(x, y) = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n}
$$

Оценка ковариации по выборке:

$$
\hat{\sigma}_{xy} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n-1}
$$

Если оценить ковариацию переменной с самой собой, какая будет формула?

## Ковариация {style="font-size: 90%"}

Ковариация переменной с самой собой - это дисперсия.

Давайте посчитаем ковариацию: функция `cov()`.

```{r}
df %>% 
  select(happyScore, GDP) %>% 
  cov()
```

По главной диагонали ковариация переменной с самой собой - дисперсия.

```{r}
df %>% 
  select(happyScore, GDP) %>% 
  var() # тот же самый результат с помощью функции var
```

Можем ли мы сделать вывод, это относительно большая ковариация между переменными или нет?

## Коэффициент корреляции {style="font-size: 90%"}

Проблема ковариации в том, что она привязана к исходной шкале и измеряется в пределах $[-\infty; \infty]$. Неплохо было бы иметь возможность нормировать. Для этого используется коэффициент корреляции.

Формула коэффициента корреляции Пирсона:

$$
\rho_{xy} = \frac{\sigma_{xy}}{\sigma_x \sigma_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n}\sum_{i = 1}^n z_{x,i} z_{y, i}
$$

Оценка коэффициента корреляции Пирсона по выборке:

$$
r_{xy} = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n - 1}\sum_{i = 1}^n z_{x,i} z_{y, i}
$$

По сути это ковариация, деленная на стандартное отклонение обеих переменных.

Коэффициент корреляции измеряется в пределах $[-1; 1]$.

## Вычисляем корреляцию

Используем функцию `cor()` базового R.

```{r}
df %>% 
  select(happyScore, GDP) %>% 
  cor() # много это или мало?
```

Посмотрим визуально на распределение данных.

```{r}
plot(df$GDP, df$happyScore)
```

Значим ли этот коэффициент корреляции?

## Тестируем значимость коэффициента корреляции {style="font-size: 85%"}

Для оценки статистической значимости коэффициента корреляции используется функция `cor.test()`.

Нулевая гипотеза - коэффициент корреляции равен нулю, альтернативная - не равен.

```{r}
cor.test(df$happyScore, df$GDP)
```

По умолчанию рассчитывается коэффициент корреляции Пирсона: оценивает связь двух нормально распределенных величин. Выявляет только линейную составляющую взаимосвязи.

## Непараметрические аналоги коэффициента корреляции Пирсона {style="font-size: 80%"}

Часто используется коэффициент корреляции Спирмена (Spearman), если в данных есть выбросы. Математика метода точно такая же как у коэффициента Пирсона, только вместо оригинальных значений используются ранги.

В целом, непараметрические критерии не зависят от формы распределения и могут оценивать связь для любых монотонных зависимостей.

```{r}
cor.test(df$happyScore, df$GDP, method = 'spearman')
```

::: {.callout-warning appearance="simple"}
Наличие значимого коэффициента корреляции ничего не говорит о причинно-следственной связи между переменными!
:::

## Непараметрические аналоги коэффициента корреляции Пирсона

Также есть коэффициент корреляции Кендалла.

```{r}
cor.test(df$happyScore, df$GDP, method = 'kendall')
```

## Сравнение различных коэффициентов корреляции

| Пирсона                               | Спирмена                                                            | Кендалла                              |
|---------------------------------------|---------------------------------------------------------------------|---------------------------------------|
| Выявляет линейную зависимость         | Выявляет любую монотонную зависимость                               | Выявляет любую монотонную зависимость |
| Количественная или интервальная шкала | шкала \>= ранговая (то есть ранговая, интервальная, количественная) | шкала \>= ранговая                    |
| Неустойчивость к выбросам             | Устойчивость к выбросам                                             | Устойчивость к выбросам               |

------------------------------------------------------------------------

Пример ситуации, когда коэффициент Спирмена более применим:

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 8
set.seed(2)
x <- rnorm(n = 20, mean = 4, sd = 4) + rnorm(20, mean = 10, sd = 4)
# x <- runif(n = 20, min = 2, max = 22)

y <- (x^6.3) + rnorm(20) 
ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + 
  geom_point(size = 3)+
  theme_bw()+
  ggtitle(label = paste0("p-value of Pearson's correlation coefficient is: ",  
                         formatC(cor.test(x,y)$p.value, 2)),
          subtitle = paste0("p-value of Spearman's correlation coefficient is: ",  
                         formatC(cor.test(x,y, method = 'spearman')$p.value, 2)))+
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 20))

```

Проведя корреляционный анализ, мы лишь ответили на вопрос "Существует ли статистически значимая связь между величинами?"

Сможем ли мы, используя это знание, **предсказать** значения одной величины, исходя из знаний другой?

# Линейная регрессия

## Какие бывают регрессионные модели?

::: incremental
-   Линейные и нелинейные

    -   Линейные

        $$
        y = b_0 + b_1x
        $$ $$
        y = b_0 + b_1x_1 + b_2x_2
        $$

        $$
        y = b_0 + b_1x_1^2 + b_2x_2^3
        $$

    -   Нелинейные

        $$
        y = b_0 + b_1^x
        $$ $$
        y = b_0^{b_1x_1+b_2x_2}
        $$

-   Простые и множественные
:::

## Линейная регрессия: формула

Линейная регрессия позволяет предсказать значение одной переменной на основании значений другой. Обе переменные должны быть количественными.

Формула для простой линейной регрессии - по сути это формула прямой из алгебры + остатки:

$$
Y = ax + b + \epsilon
$$

$Y$ - зависимая переменная, что пытаемся предсказать, $\epsilon$ - это ошибки или остатки модели (residuals) - то есть то, что наша модель не смогла предсказать.

Для более сложных моделей нотация немного меняется:

$$
Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon
$$

## Линейная регрессия

В R функция для линейной регрессии - `lm()`. Давайте попробуем предсказать значение `happyScore` на основании ВВП на душу населения (`GDP`).

```{r}
model <- lm(happyScore ~ GDP, data = df)
model
```

Мы получили коэффициенты: Intercept, GDP. Что это значит?

Как эти коэффициенты подбираются?

::: incremental
-   Метод наименьших квадратов (для простых моделей) -\> нарисовать

-   Метод максимального правдоподобия (для более сложных)
:::

## График регрессионной прямой на основании полученных коэффициентов

happyScore= Intercept + GDP \* Slope = 3.395 + GDP\*2.407

Попробуем нарисовать регрессионную прямую на нашем графике.

```{r}
ggplot(df, aes(GDP, happyScore))+
  geom_point(alpha = 0.8)+
  geom_abline(slope = model$coefficients[2], 
              intercept = model$coefficients[1], color = 'blue')
```

Немного приведем код в порядок и разобьем еще по региону, чтобы было интереснее анализировать.

## График зависимости `happyScore` от ВВП

```{r}
#| output-location: slide
ggplot(df, aes(GDP, happyScore))+
  geom_point(aes(color = region), alpha = 0.8)+
  geom_abline(slope = model$coefficients[2], 
              intercept = model$coefficients[1], color = 'blue')+
  scale_x_continuous(limits = c(0, 1.5), breaks = seq(0, 1.5, 0.25),
                     expand = c(0,0))+
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16))+
  theme_minimal()
```

С помощью `geom_abline()` задаем регрессионную прямую: подаем значения ax+b. Еще можно это сделать с помощью `geom_smooth()`, но тут задача показать, как прямую задать самостоятельно.

## Предсказание значений на основе регрессионной прямой {style="font-size: 90%"}

Функция `predict()` принимает на вход независимую переменную и предсказывает значение зависимой на основании нашей модели (подставляет в формулу).

Например, предскажем `happyScore` для `GDP` = 0.8. (Нужно подать именно датафрейм).

```{r}
predict(model, data.frame(GDP = 0.8))
```

Функция `predict()` ничего не знает о физическом смысле наших данных, поэтому мы можем предсказывать любые, даже неадекватные значения, например

```{r}
predict(model, data.frame(GDP = -5))
```

::: {.callout-important appearance="simple"}
Предсказания регрессионной модели имеют смысл **только** "внутри" графика!

Это называется интерполяция, а экстраполяция почти всегда будет неверной.
:::

## Экстраполяция и интерполяция

```{r}
#| echo: false
ggplot(df, aes(GDP, happyScore))+
  geom_point(aes(color = region), alpha = 0.8)+
  geom_abline(slope = model$coefficients[2], 
              intercept = model$coefficients[1], color = 'blue')+
  scale_x_continuous(limits = c(-1, 2.5), breaks = seq(0, 1.5, 0.25),
                     expand = c(0,0))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_vline(xintercept = 1.5, linetype = 2)+
  annotate('text', x = -0.5, y = 3, label = 'Extrapolation')+
  annotate('text', x = 2, y = 3, label = 'Extrapolation')+
  annotate('text', x = 0.75, y = 3.2, label = 'Interpolation')+
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16))+
  theme_minimal()
```

## Более внимательно посмотрим на вывод регрессии

```{r}
summary(model)
```

Что все это значит?

## Самое важное из вывода регрессии

```{r}
summary(model)$coefficients
```

p-value есть для каждого коэффициента,

$H_0: Intercept = 0, Slope = 0$.

$H_1: Intercept \neq 0, Slope \neq 0$.

Какое значение p-value для нас важнее?

::: fragment
Коэффициент наклона.
:::

Как рассчитывается значимость коэффициента наклона?

::: fragment
Рассчитывается относительно "нулевой" модели - то есть модели, в которую входит только интерсепт.
:::

## Коэффициент детерминации - мера качества модели

$R^2$ - коэффициент детерминации. Описывает какую долю дисперсии зависимой переменной объясняет модель.

$$
R^2 = \frac{\color{blue}{SS_{r}}}{SS_{t}}
$$

-   Измеряется от $[0, 1]$.

-   $R^2 = r^2$ - для простой линейной регрессии коэффициент детерминации - квадрат коэффициента корреляции Пирсона.

## Допущения линейной регрессии

-   Линейная связь

-   Независимость

-   Гомогенность дисперсий

-   Отсутствие коллинеарности предикторов (для множественной регрессии)

-   Нормальное распределение ошибок -\> проиллюстрировать другие графики в R

    ```{r}
    plot(model)
    ```

## Линейность связи

Нелинейность связи видно на графиках остатков.

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6
library(patchwork)
set.seed(39484)
x <- rnorm(100, 10, 3)
y <- (x^2.4) + rnorm(100, 0, 100)
pl_1 <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + 
  geom_point()+theme_bw()
lm1 <- lm(y ~ x)
pl_1res <- ggplot(data.frame(fit = fitted(lm1), 
                             res = residuals(lm1)), 
                  aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  xlab("Fitted") + 
  ylab("Residuals")+theme_bw()
x2 <- runif(100, 1, 8)
y2 <- sin(x2) + 2 * x2 + rnorm(100)
pl_2 <- ggplot(data.frame(x = x2, y = y2), aes(x = x, y = y)) +
        geom_point() +theme_bw()
lm2 <- lm(y2 ~ x2)
pl_2res <- ggplot(data.frame(fit = fitted(lm2), 
                             res = residuals(lm2)), 
                             aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  xlab("Fitted") + 
  ylab("Residuals")+theme_bw() 
(pl_1 / pl_1res) | (pl_2 / pl_2res)
```
:::

::: {.column width="50%"}
Проверка на линейность связи:

-   График зависимости y от x (и от других переменных, не включенных в модель).

-   График остатков от предсказанных значений.

Что делать с нелинейностью данных?

-   Добавить неучтенные переменные или взаимодействия

-   Применить линеаризующее преобразование (Осторожно!)

-   Применить обобщенную линейную модель с другой функцией связи (GLM)
:::
:::

## График остатков для наших данных

```{r}
#| output-location: slide
ggplot(data.frame(fit = fitted(model), 
                             res = residuals(model)), 
                  aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = 2) + 
  xlab("Fitted") + 
  ylab("Residuals")+
  theme_bw()
```

## Независимость данных

Каждое значение $y_i$ должно быть независимо от любого другого $y_j$.

Это возможно проконтролировать на этапе сбора данных

Наиболее частые источники зависимостей:

-   Псевдоповторности (когда измерили один и тот же объект несколько раз)

-   Временные и пространственные автокорреляции

-   Неучтенные переменные

## Нормальное распределение ошибок

Можно проверить с помощью `qqPlot()`.

```{r}
car::qqPlot(model$residuals, id = FALSE)
```

## Проверка на гомогенность дисперсий (гомоскедастичность)

Многие тесты чувствительны к гетероскедастичности.

Лучший способ проверки на гомогенность дисперсий --- график остатков от предсказанных значений.

## Проверка на гомогенность дисперсий

[![взято из презентации Марины Варфоломеевой](images/image-1051634690.png)](https://varmara.github.io/linmodr/07_model_description_and_validation.html#96)

## Множественная линейная регрессия

Можно использовать несколько предикторов для объяснения зависимой переменной.

Больше предикторов - лучше модель?

```{r}
model_income <- lm(happyScore ~ income_inequality + GDP, data = df)
summary(model_income)
```

## Множественная линейная регрессия

Переменная income_inequality оказалась незначимым предиктором. Возможно, есть смысл включить все возможные переменные в модель?

## Проблема мультиколлинеарности во множественной линейной регрессии {style="font-size: 80%"}

Для множественной линейной регрессии очень плохо, когда независимые переменные коррелируют друг с другом. Чтобы оценить, насколько у нас выражена автокорреляция, построим плот корреляций (пакет `corrplot`, функция `corrplot()`).

```{r}
corrplot::corrplot(cor(df %>% select(where(is.numeric))))
```

Большинство переменных так или иначе коррелируют с GDP, следовательно, включать их в модель нет особого смысла.

## Линейная регрессия - частный случай общей линейной модели

И вообще большинство статистических тестов -- это частный случай общей или обобщенной линейной модели.

Но про это подробнее в другой раз.

# Вернемся к ANOVA

## Ограничения ANOVA

-   Нормальность распределения - под вопросом (тоже самое, что и про t-test).

-   Равенство дисперсий - необязательно, если дизайн сбалансированный.

-   Нормальность распределения остатков - очень важно.

-   Независимость наблюдений.

## Что такое остатки в ANOVA?

Дисперсионный анализ можно воспринимать как частный случай линейной модели, когда зависимая переменная количественная, а независимая номинативная.

Объяснение на рисунке.

## Проверка на нормальность распределения остатков

Вспомним, что были за данные и проведем однофакторную анову:

```{r}
diet <- readr::read_csv("https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/stcp-Rdataset-Diet.csv")
diet <- diet %>%
  mutate(weight_loss = weight6weeks - pre.weight,
         Dietf = factor(Diet, labels = LETTERS[1:3]),
         Person = factor(Person)) %>%
  drop_na()
fit_diet <- aov(weight_loss ~ Dietf, data = diet)
```

## Проверка на нормальность распределения остатков

```{r}
hist(residuals(fit_diet))

```

Распределение похоже на нормальное.

------------------------------------------------------------------------

```{r}
car::qqPlot(residuals(fit_diet))
```

## Независимость наблюдений

Актуальны все те же правила, что были для линейной регрессии и т-теста.

## Спасибо за внимание!

Если понравилось, переходите по [ссылке](https://www.tinkoff.ru/rm/ubogoeva.elena1/TSRBI31474):

![](images/qrcode.png)
