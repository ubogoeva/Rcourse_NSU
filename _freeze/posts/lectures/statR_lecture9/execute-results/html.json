{
  "hash": "97116b0407a818409dcf33bebdf3b2b5",
  "result": {
    "markdown": "---\ntitle: \"Язык программирования R для анализа данных: лекция 9\"\nsubtitle: 'Ограничения ANOVA, линейная регрессия'\nauthor: \"Elena U\"\n#date: \"Created on 01 April, 2023\"\nexecute:\n  echo: true\n  output: true\nformat: \n  revealjs:\n    slide-number: c/t\n    show-slide-number: all\n    # mainfont: Arial\n    # fontsize: 14px\n    theme: [default, custom.scss]\n    chalkboard: \n      buttons: true\n    # theme: [serif]\n    # mouse-wheel: true\n    auto-play-media: true\n    width: 1280\n    height: 720\n    fig-dpi: 300\n    # logo: figures/icg.png\nrevealjs-plugins:\n  - pointer\neditor: visual\ndraft: true\n---\n\n\n## План лекции\n\n-   ::: {.fragment .fade-in-then-out}\n    Ограничения ANOVA\n    :::\n\n-   Ковариация и корреляция\n\n-   Простая линейная регрессия: формула, реализация в R, интерпретация\n\n-   ::: {.fragment .highlight-blue}\n    Ограничения линейной регрессии (в т.ч. нормальность распределения остатков)\n    :::\n\n\n```{=html}\n<!-- -->\n```\n\n-   Множественная линейная регрессия\n\n-   ::: {.fragment .fade-in-then-out}\n    Ограничения ANOVA\n    :::\n\n# Ковариация и корреляция\n\n## Данные для работы\n\nДатасет с [каггла](https://www.kaggle.com/datasets/levyedgar44/income-and-happiness-correction) о связи ВВП (GDP) на душу населения и счастья.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- read_csv('https://raw.githubusercontent.com/ubogoeva/Rcourse_NSU/master/posts/lectures/data/happyscore_income.csv')\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 11\n  country...1 adjusted_satisfaction avg_satisfaction std_satisfaction avg_income\n  <chr>                       <dbl>            <dbl>            <dbl>      <dbl>\n1 Armenia                        37              4.9             2.42      2097.\n2 Angola                         26              4.3             3.19      1449.\n3 Argentina                      60              7.1             1.91      7101.\n4 Austria                        59              7.2             2.11     19457.\n5 Australia                      65              7.6             1.8      19917 \n6 Azerbaijan                     46              5.8             2.27      3382.\n# ℹ 6 more variables: median_income <dbl>, income_inequality <dbl>,\n#   region <chr>, happyScore <dbl>, GDP <dbl>, country...11 <chr>\n```\n:::\n\n```{.r .cell-code}\ndf <- df %>% \n  rename(country = country...1) %>% \n  select(!country...11)\n```\n:::\n\n\n## Посмотрим на данные\n\nИспользуя базовую generic-функцию `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   country          adjusted_satisfaction avg_satisfaction std_satisfaction\n Length:111         Min.   :19.00         Min.   :2.500    Min.   :1.380   \n Class :character   1st Qu.:40.00         1st Qu.:5.100    1st Qu.:1.910   \n Mode  :character   Median :48.00         Median :6.000    Median :2.130   \n                    Mean   :48.73         Mean   :5.937    Mean   :2.125   \n                    3rd Qu.:57.00         3rd Qu.:7.000    3rd Qu.:2.330   \n                    Max.   :74.00         Max.   :8.500    Max.   :3.190   \n   avg_income      median_income     income_inequality    region         \n Min.   :  572.9   Min.   :  415.5   Min.   :24.21     Length:111        \n 1st Qu.: 1519.4   1st Qu.: 1167.7   1st Qu.:32.18     Class :character  \n Median : 3889.3   Median : 2647.0   Median :36.48     Mode  :character  \n Mean   : 6442.8   Mean   : 5186.0   Mean   :38.42                       \n 3rd Qu.: 7867.4   3rd Qu.: 6581.1   3rd Qu.:43.38                       \n Max.   :26182.3   Max.   :22240.2   Max.   :63.73                       \n   happyScore         GDP        \n Min.   :2.839   Min.   :0.0153  \n 1st Qu.:4.568   1st Qu.:0.5099  \n Median :5.268   Median :0.9186  \n Mean   :5.422   Mean   :0.8419  \n 3rd Qu.:6.392   3rd Qu.:1.1495  \n Max.   :7.587   Max.   :1.5639  \n```\n:::\n:::\n\n\nБольше всего нас будут интересовать переменные `happyScore` и `GDP`.\n\n## Посмотрим на данные\n\nС помощью функции `skim()` из пакета `skimr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(df)\n```\n:::\n\n\n![](images/image-419789197.png)\n\n(Аутпут очень большой, поэтому прикрепила скрином).\n\n## Ковариация\n\nСамая простая мера связи между двумя количественными переменными --- это ковариация. Если ковариация положительная, то чем больше одна переменная, тем больше другая переменная. При отрицательной ковариации наоборот: чем больше одна переменная, тем меньше другая. -\\> проиллюстрировать на графике\n\nФормула ковариации:\n\n$$\n\\sigma_{xy} = cov(x, y) = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{n}\n$$\n\nОценка ковариации по выборке:\n\n$$\n\\hat{\\sigma}_{xy} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\n$$\n\nЕсли оценить ковариацию переменной с самой собой, какая будет формула?\n\n## Ковариация  {style=\"font-size: 90%\"}\n\nКовариация переменной с самой собой - это дисперсия.\n\nДавайте посчитаем ковариацию: функция `cov()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% \n  select(happyScore, GDP) %>% \n  cov()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           happyScore       GDP\nhappyScore  1.3942899 0.3615849\nGDP         0.3615849 0.1502264\n```\n:::\n:::\n\n\nПо главной диагонали ковариация переменной с самой собой - дисперсия.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% \n  select(happyScore, GDP) %>% \n  var() # тот же самый результат с помощью функции var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           happyScore       GDP\nhappyScore  1.3942899 0.3615849\nGDP         0.3615849 0.1502264\n```\n:::\n:::\n\n\nМожем ли мы сделать вывод, это относительно большая ковариация между переменными или нет?\n\n## Коэффициент корреляции {style=\"font-size: 90%\"}\n\nПроблема ковариации в том, что она привязана к исходной шкале и измеряется в пределах $[-\\infty; \\infty]$. Неплохо было бы иметь возможность нормировать. Для этого используется коэффициент корреляции.\n\nФормула коэффициента корреляции Пирсона:\n\n$$\n\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i = 1}^n(x_i - \\overline{x})^2}\\sqrt{\\sum_{i = 1}^n(y_i - \\overline{y})^2}} = \\frac{1}{n}\\sum_{i = 1}^n z_{x,i} z_{y, i}\n$$\n\nОценка коэффициента корреляции Пирсона по выборке:\n\n$$\nr_{xy} = \\frac{\\hat{\\sigma}_{xy}}{\\hat{\\sigma}_x \\hat{\\sigma}_y} = \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i = 1}^n(x_i - \\overline{x})^2}\\sqrt{\\sum_{i = 1}^n(y_i - \\overline{y})^2}} = \\frac{1}{n - 1}\\sum_{i = 1}^n z_{x,i} z_{y, i}\n$$\n\nПо сути это ковариация, деленная на стандартное отклонение обеих переменных.\n\nКоэффициент корреляции измеряется в пределах $[-1; 1]$.\n\n## Вычисляем корреляцию\n\nИспользуем функцию `cor()` базового R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>% \n  select(happyScore, GDP) %>% \n  cor() # много это или мало?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           happyScore       GDP\nhappyScore  1.0000000 0.7900609\nGDP         0.7900609 1.0000000\n```\n:::\n:::\n\n\nПосмотрим визуально на распределение данных.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(df$GDP, df$happyScore)\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-7-1.png){width=3000}\n:::\n:::\n\n\nЗначим ли этот коэффициент корреляции?\n\n## Тестируем значимость коэффициента корреляции {style=\"font-size: 85%\"}\n\nДля оценки статистической значимости коэффициента корреляции используется функция `cor.test()`.\n\nНулевая гипотеза - коэффициент корреляции равен нулю, альтернативная - не равен.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$happyScore, df$GDP)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$happyScore and df$GDP\nt = 13.455, df = 109, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7079171 0.8511169\nsample estimates:\n      cor \n0.7900609 \n```\n:::\n:::\n\n\nПо умолчанию рассчитывается коэффициент корреляции Пирсона: оценивает связь двух нормально распределенных величин. Выявляет только линейную составляющую взаимосвязи.\n\n## Непараметрические аналоги коэффициента корреляции Пирсона {style=\"font-size: 80%\"}\n\nЧасто используется коэффициент корреляции Спирмена (Spearman), если в данных есть выбросы. Математика метода точно такая же как у коэффициента Пирсона, только вместо оригинальных значений используются ранги.\n\nВ целом, непараметрические критерии не зависят от формы распределения и могут оценивать связь для любых монотонных зависимостей.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$happyScore, df$GDP, method = 'spearman')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  df$happyScore and df$GDP\nS = 47026, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7936732 \n```\n:::\n:::\n\n\n::: {.callout-warning appearance=\"simple\"}\nНаличие значимого коэффициента корреляции ничего не говорит о причинно-следственной связи между переменными!\n:::\n\n## Непараметрические аналоги коэффициента корреляции Пирсона\n\nТакже есть коэффициент корреляции Кендалла.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$happyScore, df$GDP, method = 'kendall')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKendall's rank correlation tau\n\ndata:  df$happyScore and df$GDP\nz = 9.3551, p-value < 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.6013104 \n```\n:::\n:::\n\n\n## Сравнение различных коэффициентов корреляции\n\n| Пирсона                               | Спирмена                                                            | Кендалла                              |\n|---------------------------------------|---------------------------------------------------------------------|---------------------------------------|\n| Выявляет линейную зависимость         | Выявляет любую монотонную зависимость                               | Выявляет любую монотонную зависимость |\n| Количественная или интервальная шкала | шкала \\>= ранговая (то есть ранговая, интервальная, количественная) | шкала \\>= ранговая                    |\n| Неустойчивость к выбросам             | Устойчивость к выбросам                                             | Устойчивость к выбросам               |\n\n------------------------------------------------------------------------\n\nПример ситуации, когда коэффициент Спирмена более применим:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-11-1.png){width=2400}\n:::\n:::\n\n\nПроведя корреляционный анализ, мы лишь ответили на вопрос \"Существует ли статистически значимая связь между величинами?\"\n\nСможем ли мы, используя это знание, **предсказать** значения одной величины, исходя из знаний другой?\n\n# Линейная регрессия\n\n## Какие бывают регрессионные модели?\n\n::: incremental\n-   Линейные и нелинейные\n\n    -   Линейные\n\n        $$\n        y = b_0 + b_1x\n        $$\n        $$\n        y = b_0 + b_1x_1 + b_2x_2\n        $$\n\n        $$\n        y = b_0 + b_1x_1^2 + b_2x_2^3\n        $$\n\n    -   Нелинейные\n\n        $$\n        y = b_0 + b_1^x\n        $$\n        $$\n        y = b_0^{b_1x_1+b_2x_2}\n        $$\n\n-   Простые и множественные\n:::\n\n## Линейная регрессия: формула\n\nЛинейная регрессия позволяет предсказать значение одной переменной на основании значений другой. Обе переменные должны быть количественными.\n\nФормула для простой линейной регрессии - по сути это формула прямой из алгебры + остатки:\n\n$$\nY = ax + b + \\epsilon\n$$\n\n$Y$ - зависимая переменная, что пытаемся предсказать, $\\epsilon$ - это ошибки или остатки модели (residuals) - то есть то, что наша модель не смогла предсказать.\n\nДля более сложных моделей нотация немного меняется:\n\n$$\nY = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon\n$$\n\n## Линейная регрессия\n\nВ R функция для линейной регрессии - `lm()`. Давайте попробуем предсказать значение `happyScore` на основании ВВП на душу населения (`GDP`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(happyScore ~ GDP, data = df)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ GDP, data = df)\n\nCoefficients:\n(Intercept)          GDP  \n      3.395        2.407  \n```\n:::\n:::\n\n\nМы получили коэффициенты: Intercept, GDP. Что это значит?\n\nКак эти коэффициенты подбираются?\n\n::: incremental\n-   Метод наименьших квадратов (для простых моделей) -\\> нарисовать\n\n-   Метод максимального правдоподобия (для более сложных)\n:::\n\n## График регрессионной прямой на основании полученных коэффициентов\n\nhappyScore= Intercept + GDP \\* Slope = 3.395 + GDP\\*2.407\n\nПопробуем нарисовать регрессионную прямую на нашем графике.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df, aes(GDP, happyScore))+\n  geom_point(alpha = 0.8)+\n  geom_abline(slope = model$coefficients[2], \n              intercept = model$coefficients[1], color = 'blue')\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-13-1.png){width=3000}\n:::\n:::\n\n\nНемного приведем код в порядок и разобьем еще по региону, чтобы было интереснее анализировать.\n\n## График зависимости `happyScore` от ВВП\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(df, aes(GDP, happyScore))+\n  geom_point(aes(color = region), alpha = 0.8)+\n  geom_abline(slope = model$coefficients[2], \n              intercept = model$coefficients[1], color = 'blue')+\n  scale_x_continuous(limits = c(0, 1.5), breaks = seq(0, 1.5, 0.25),\n                     expand = c(0,0))+\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16))+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-14-1.png){width=3000}\n:::\n:::\n\n\nС помощью `geom_abline()` задаем регрессионную прямую: подаем значения ax+b. Еще можно это сделать с помощью `geom_smooth()`, но тут задача показать, как прямую задать самостоятельно.\n\n## Предсказание значений на основе регрессионной прямой {style=\"font-size: 90%\"}\n\nФункция `predict()` принимает на вход независимую переменную и предсказывает значение зависимой на основании нашей модели (подставляет в формулу).\n\nНапример, предскажем `happyScore` для `GDP` = 0.8. (Нужно подать именно датафрейм).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(model, data.frame(GDP = 0.8))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n5.321038 \n```\n:::\n:::\n\n\nФункция `predict()` ничего не знает о физическом смысле наших данных, поэтому мы можем предсказывать любые, даже неадекватные значения, например\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(model, data.frame(GDP = -5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1 \n-8.639174 \n```\n:::\n:::\n\n\n::: {.callout-important appearance=\"simple\"}\nПредсказания регрессионной модели имеют смысл **только** \"внутри\" графика!\n\nЭто называется интерполяция, а экстраполяция почти всегда будет неверной.\n:::\n\n## Экстраполяция и интерполяция\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-17-1.png){width=3000}\n:::\n:::\n\n\n## Более внимательно посмотрим на вывод регрессии\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ GDP, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05142 -0.47761 -0.02728  0.60008  1.53001 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.3955     0.1657   20.50   <2e-16 ***\nGDP           2.4069     0.1789   13.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7272 on 109 degrees of freedom\nMultiple R-squared:  0.6242,\tAdjusted R-squared:  0.6207 \nF-statistic:   181 on 1 and 109 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nЧто все это значит?\n\n## Самое важное из вывода регрессии\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 3.395491  0.1656664 20.49596 3.459016e-39\nGDP         2.406933  0.1788837 13.45530 6.570905e-25\n```\n:::\n:::\n\n\np-value есть для каждого коэффициента,\n\n$H_0: Intercept = 0, Slope = 0$.\n\n$H_1: Intercept \\neq 0, Slope \\neq 0$.\n\nКакое значение p-value для нас важнее?\n\n::: fragment\nКоэффициент наклона.\n:::\n\nКак рассчитывается значимость коэффициента наклона?\n\n::: fragment\nРассчитывается относительно \"нулевой\" модели - то есть модели, в которую входит только интерсепт.\n:::\n\n## Коэффициент детерминации - мера качества модели\n\n$R^2$ - коэффициент детерминации. Описывает какую долю дисперсии зависимой переменной объясняет модель.\n\n$$\nR^2 = \\frac{\\color{blue}{SS_{r}}}{SS_{t}}\n$$\n\n-   Измеряется от $[0, 1]$.\n\n-   $R^2 = r^2$ - для простой линейной регрессии коэффициент детерминации - квадрат коэффициента корреляции Пирсона.\n\n## Допущения линейной регрессии\n\n-   Линейная связь\n\n-   Независимость\n\n-   Гомогенность дисперсий\n\n-   Отсутствие коллинеарности предикторов (для множественной регрессии)\n\n-   Нормальное распределение ошибок -\\> проиллюстрировать другие графики в R\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    plot(model)\n    ```\n    \n    ::: {.cell-output-display}\n    ![](statR_lecture9_files/figure-revealjs/unnamed-chunk-20-1.png){width=3000}\n    :::\n    \n    ::: {.cell-output-display}\n    ![](statR_lecture9_files/figure-revealjs/unnamed-chunk-20-2.png){width=3000}\n    :::\n    \n    ::: {.cell-output-display}\n    ![](statR_lecture9_files/figure-revealjs/unnamed-chunk-20-3.png){width=3000}\n    :::\n    \n    ::: {.cell-output-display}\n    ![](statR_lecture9_files/figure-revealjs/unnamed-chunk-20-4.png){width=3000}\n    :::\n    :::\n\n\n## Линейность связи\n\nНелинейность связи видно на графиках остатков.\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-21-1.png){width=1800}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nПроверка на линейность связи:\n\n-   График зависимости y от x (и от других переменных, не включенных в модель).\n\n-   График остатков от предсказанных значений.\n\nЧто делать с нелинейностью данных?\n\n-   Добавить неучтенные переменные или взаимодействия\n\n-   Применить линеаризующее преобразование (Осторожно!)\n\n-   Применить обобщенную линейную модель с другой функцией связи (GLM)\n:::\n:::\n\n## График остатков для наших данных\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(data.frame(fit = fitted(model), \n                             res = residuals(model)), \n                  aes(x = fit, y = res)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2) + \n  xlab(\"Fitted\") + \n  ylab(\"Residuals\")+\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-22-1.png){width=3000}\n:::\n:::\n\n\n## Независимость данных\n\nКаждое значение $y_i$ должно быть независимо от любого другого $y_j$.\n\nЭто возможно проконтролировать на этапе сбора данных\n\nНаиболее частые источники зависимостей:\n\n-   Псевдоповторности (когда измерили один и тот же объект несколько раз)\n\n-   Временные и пространственные автокорреляции\n\n-   Неучтенные переменные\n\n## Нормальное распределение ошибок\n\nМожно проверить с помощью `qqPlot()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::qqPlot(model$residuals, id = FALSE)\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-23-1.png){width=3000}\n:::\n:::\n\n\n## Проверка на гомогенность дисперсий (гомоскедастичность)\n\nМногие тесты чувствительны к гетероскедастичности.\n\nЛучший способ проверки на гомогенность дисперсий --- график остатков от предсказанных значений.\n\n## Проверка на гомогенность дисперсий\n\n[![взято из презентации Марины Варфоломеевой](images/image-1051634690.png)](https://varmara.github.io/linmodr/07_model_description_and_validation.html#96)\n\n## Множественная линейная регрессия\n\nМожно использовать несколько предикторов для объяснения зависимой переменной.\n\nБольше предикторов - лучше модель?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_income <- lm(happyScore ~ income_inequality + GDP, data = df)\nsummary(model_income)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ income_inequality + GDP, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.09360 -0.45110  0.01026  0.54468  1.43782 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.038567   0.416302   7.299 5.17e-11 ***\nincome_inequality 0.008124   0.008692   0.935    0.352    \nGDP               2.460162   0.187829  13.098  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7276 on 108 degrees of freedom\nMultiple R-squared:  0.6272,\tAdjusted R-squared:  0.6203 \nF-statistic: 90.85 on 2 and 108 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Множественная линейная регрессия\n\nПеременная income_inequality оказалась незначимым предиктором. Возможно, есть смысл включить все возможные переменные в модель?\n\n## Проблема мультиколлинеарности во множественной линейной регрессии {style=\"font-size: 80%\"}\n\nДля множественной линейной регрессии очень плохо, когда независимые переменные коррелируют друг с другом. Чтобы оценить, насколько у нас выражена автокорреляция, построим плот корреляций (пакет `corrplot`, функция `corrplot()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cor(df %>% select(where(is.numeric))))\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-25-1.png){width=3000}\n:::\n:::\n\n\nБольшинство переменных так или иначе коррелируют с GDP, следовательно, включать их в модель нет особого смысла.\n\n## Линейная регрессия - частный случай общей линейной модели\n\nИ вообще большинство статистических тестов -- это частный случай общей или обобщенной линейной модели.\n\nНо про это подробнее в другой раз.\n\n# Вернемся к ANOVA\n\n## Ограничения ANOVA\n\n-   Нормальность распределения - под вопросом (тоже самое, что и про t-test).\n\n-   Равенство дисперсий - необязательно, если дизайн сбалансированный.\n\n-   Нормальность распределения остатков - очень важно.\n\n-   Независимость наблюдений.\n\n## Что такое остатки в ANOVA?\n\nДисперсионный анализ можно воспринимать как частный случай линейной модели, когда зависимая переменная количественная, а независимая номинативная.\n\nОбъяснение на рисунке.\n\n## Проверка на нормальность распределения остатков\n\nВспомним, что были за данные и проведем однофакторную анову:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiet <- readr::read_csv(\"https://raw.githubusercontent.com/Pozdniakov/tidy_stats/master/data/stcp-Rdataset-Diet.csv\")\ndiet <- diet %>%\n  mutate(weight_loss = weight6weeks - pre.weight,\n         Dietf = factor(Diet, labels = LETTERS[1:3]),\n         Person = factor(Person)) %>%\n  drop_na()\nfit_diet <- aov(weight_loss ~ Dietf, data = diet)\n```\n:::\n\n\n## Проверка на нормальность распределения остатков\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(residuals(fit_diet))\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-27-1.png){width=3000}\n:::\n:::\n\n\nРаспределение похоже на нормальное.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::qqPlot(residuals(fit_diet))\n```\n\n::: {.cell-output-display}\n![](statR_lecture9_files/figure-revealjs/unnamed-chunk-28-1.png){width=3000}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 48 15\n```\n:::\n:::\n\n\n## Независимость наблюдений\n\nАктуальны все те же правила, что были для линейной регрессии и т-теста.\n\n## Спасибо за внимание!\n\nЕсли понравилось, переходите по [ссылке](https://www.tinkoff.ru/rm/ubogoeva.elena1/TSRBI31474):\n\n![](images/qrcode.png)\n",
    "supporting": [
      "statR_lecture9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}