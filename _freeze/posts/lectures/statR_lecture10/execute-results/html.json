{
  "hash": "7d55f9f791405e02738058b356ce7ee3",
  "result": {
    "markdown": "---\ntitle: \"Язык программирования R для анализа данных: лекция 10\"\nsubtitle: 'Множественная линейная регрессия'\nauthor: \"Elena U\"\n#date: \"Created on 01 April, 2023\"\nexecute:\n  echo: true\n  output: true\nformat: \n  revealjs:\n    slide-number: c/t\n    show-slide-number: all\n    # mainfont: Arial\n    # fontsize: 14px\n    theme: [default, custom.scss]\n    chalkboard: \n      buttons: true\n    # theme: [serif]\n    # mouse-wheel: true\n    auto-play-media: true\n    width: 1280\n    height: 720\n    fig-dpi: 300\n    # logo: figures/icg.png\nrevealjs-plugins:\n  - pointer\neditor: visual\ndraft: true\n---\n\n\n\n\n## Вспомним материал предыдущей лекции {style=\"auto-animate\"}\n\n::: incremental\n-   Что такое ковариация? Что такое коэффициент корреляции?\n\n    ::: {.fragment .fade-in}\n    **Ковариация** -- это мера совместной изменчивости двух случайных величин\n\n    **Коэффициент корреляции** -- это статистическая мера степени, в которой изменения значения одной переменной предсказывают изменение значения другой.\n    :::\n\n-   Какие задачи можно решить с помощью линейной регрессии?\n\n    ::: {.fragment .fade-in}\n    Предсказание значений одной переменной на основании другой\n    :::\n\n-   Коэффициент детерминации\n\n-   Ограничения линейной регрессии\n:::\n\n## План лекции\n\n-   Что такое коэффициент детерминации и как он выводится?\n\n-   Множественная линейная регрессия\n\n-   Проблема мультиколлинеарности\n\n-   Отбор моделей\n\n-   Ограничения линейной регрессии\n\nЗагрузим нужные библиотеки:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\n```\n:::\n\n\n## Коэффициент детерминации: формула\n\nОбозначается как $R^2$ -- доля объясненной дисперсии.\n\n$$\nR^2 = \\frac{SSR}{SST}\n$$\n\n$SSR = \\sum_i{(\\hat{y_i}-\\overline{y})^2}$ - объясненная моделью дисперсия.\n\n$SSE = \\sum{(y_i - \\hat{y_i})^2}$ - необъясненная дисперсия.\n\n$SST = SSR + SSE = \\sum_i{(y_i-\\overline{y})^2}$ - общая дисперсия.\n\nОбъясненную моделью дисперсию можно вычислить как разницу между общей дисперсией и необъясненной дисперсией ($SSR = SST - SSE$).\n\nРазберем на рисунке:\n\n## Коэффициент детерминации: график\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture10_files/figure-revealjs/unnamed-chunk-3-1.png){width=3000}\n:::\n:::\n\n\n## Коэффициент детерминации: расчет\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(\n  x=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),\n  y=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3))\nsst <- sum((df$y - mean(df$y))^2) # общая сумма квадратов, SST\nssr <- sst - sum((df$y - predict(model, data.frame(x = df$x)))^2)\nr2 <- ssr / sst\nr2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6132867\n```\n:::\n:::\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nsummary(lm(y ~ x, data = df)) # все сходится\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5482 -0.8037  0.1186  0.6186  1.8852 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   0.5813     0.9647   0.603   0.5658  \nx             0.7778     0.2334   3.332   0.0126 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.19 on 7 degrees of freedom\nMultiple R-squared:  0.6133,\tAdjusted R-squared:  0.558 \nF-statistic:  11.1 on 1 and 7 DF,  p-value: 0.01256\n```\n:::\n:::\n\n\n## Коэффициент детерминации\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture10_files/figure-revealjs/unnamed-chunk-6-1.png){width=3000}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n![](images/image-1678901130.png)\n\n# Множественная линейная регрессия\n\n## Как собирать данные?\n\nВажно помнить, что Garbage in - Garbage out.\n\n![](/posts/lectures/images/image-1617963202.png){alt=\"Проблема мультиколлинеарности\"}\n\n-   Независимость наблюдений.\n\n-   Проверка на наличие скрытых (группирующих) переменных.\n\n-   Учет пространственных и временных автокорреляций.\n\n## Данные для работы\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhappy_score_data <- read_csv('https://raw.githubusercontent.com/ubogoeva/Rcourse_NSU/master/posts/lectures/data/happyscore_income.csv')\nhappy_score_data <- happy_score_data %>% \n  rename(country = country...1) %>% \n  select(!country...11)\nhead(happy_score_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n  country    adjusted_satisfaction avg_satisfaction std_satisfaction avg_income\n  <chr>                      <dbl>            <dbl>            <dbl>      <dbl>\n1 Armenia                       37              4.9             2.42      2097.\n2 Angola                        26              4.3             3.19      1449.\n3 Argentina                     60              7.1             1.91      7101.\n4 Austria                       59              7.2             2.11     19457.\n5 Australia                     65              7.6             1.8      19917 \n6 Azerbaijan                    46              5.8             2.27      3382.\n# ℹ 5 more variables: median_income <dbl>, income_inequality <dbl>,\n#   region <chr>, happyScore <dbl>, GDP <dbl>\n```\n:::\n:::\n\n\nДатасет с каггла о связи ВВП и счастья населения.\n\n## Формулировка множественной линейной регрессии\n\n$$\nY = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon\n$$\n\nПопробуем предсказать `happyScore`, основываясь на остальных переменных и выберем лучшую модель.\n\n## Смысл коэффициентов во множественной линейной модели\n\nИнтерсепт - значение зависимой переменной, когда все независимые равны нулю.\n\n**Коэффициент перед предиктором**: показывает на сколько единиц изменится значение зависимой переменной в случае, если значение **этого** предиктора изменится на единицу, а все **другие** показатели не изменятся.\n\n## Геометрический смысл\n\nДля двух коэффициентов теперь фитим (подгоняем) плоскость в трехмерном пространстве.\n\n$Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon$\n\n![](/posts/lectures/images/image-1714685837.png){width=\"471\"}\n\nДля большего количества предикторов невозможно нарисовать :(\n\n## Нотация формул линейных моделей в R\n\n+-----------------------------------------------------------+--------------+\n| Модель                                                    | Формула      |\n+===========================================================+==============+\n| Простая линейная регрессия                                | Y \\~ X       |\n+-----------------------------------------------------------+--------------+\n| Простая линейная регрессия, без интерсепта b~0~           | Y \\~ -1 + X  |\n+-----------------------------------------------------------+--------------+\n| Уменьшенная простая линейная регрессия (только интерсепт) | Y \\~ 1 \\     |\n|                                                           | Y \\~ 1 - X   |\n+-----------------------------------------------------------+--------------+\n| Множественная линейная регрессия                          | Y \\~ X1 + X2 |\n+-----------------------------------------------------------+--------------+\n\n## Нотация формул линейных моделей в R\n\n+-----------------+---------------------------------------------------------------------------+\n| Элемент формулы | Значение                                                                  |\n+=================+===========================================================================+\n| :               | Взаимодействие предикторов\\                                               |\n|                 | Y \\~ X1 + X2 + X1:X2                                                      |\n+-----------------+---------------------------------------------------------------------------+\n| \\*              | Все возможные взаимодействия                                              |\n|                 |                                                                           |\n|                 | Y \\~ X1\\*X2 тоже самое, что и\\                                            |\n|                 | Y \\~ X1+X2 + X1:X2                                                        |\n+-----------------+---------------------------------------------------------------------------+\n| .               | Y \\~ .                                                                    |\n|                 |                                                                           |\n|                 | В правой части формулы записываются все переменные из датафрейма, кроме Y |\n+-----------------+---------------------------------------------------------------------------+\n\n## Условия применимости множественной линейной регрессии {style=\"font-size: 90%\"}\n\n1.  Линейная связь (=отсутствие паттерна на графике остатков).\n\n2.  Независимость наблюдений.\n\n3.  Гомогенность дисперсий.\n\n4.  Нормально распределение ошибок (остатков).\n\n5.  Отсутствие влиятельных наблюдений.\n\n6.  **Отсутствие коллинеарности предикторов (для множественной регрессии)**\n\nВсе условия совпадают с требованиями к простой линейной регрессии + проверка на коллинеарность.\n\n::: callout-note\nВообще перечисленные выше условия применимости совпадают и с многими другими статистическими тестами (и это не случайно!)\n:::\n\n## График остатков\n\nТакой вариант графика позволяет оценивать паттерны распределения остатков для нескольких независимых переменных.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statR_lecture10_files/figure-revealjs/unnamed-chunk-8-1.png){width=3000}\n:::\n:::\n\n\n## Влиятельные наблюдения - выбросы, аутлайеры\n\nОцениваются с помощью расстояния Кука\n\n$$\nD_i = \\frac{\\sum{( \\color{blue}{\\hat{y_{j}}} - \\color{red}{\\hat{y}_{j(i)}})^2}}{p \\; MS_e}\n$$\n\n$\\color{blue}{\\hat{y_j}}$ --- значение, предсказанное полной моделью.\n\n$\\color{red}{\\hat{y}_{j(i)}}$ --- значение, предсказанное моделью, построенной без учета $i$-го значения предиктора\n\n$p$ --- количество параметров в модели\n\n$MS_{e}$ --- среднеквадратичная ошибка модели ($\\hat\\sigma^2$)\n\nПороговое значение можно рассчитать по формуле, но вообще существуют разные способы выбора порога:\n\n$$\nCutoff = \\frac{4}{ n - p}\n$$\n\nФормулы могут быть разные еще и потому, учитывают они интерсепт как оцениваемый параметр или нет.\n\n## Давайте построим модель\n\nПредсказание цены дома в зависимости от всех количественных переменных. Сначала отберем все численные переменные (можно строить модель и с дискретными предикторами, но пока построим так) и включим их в модель.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhappy_score_data_numeric <- happy_score_data %>% select(where(is.numeric))\nmodel_full <- lm(happyScore ~ ., happy_score_data_numeric)\nsummary(model_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ ., data = happy_score_data_numeric)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.23723 -0.28006  0.01552  0.28669  1.07112 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)\n(Intercept)            2.2183187  2.2722660   0.976    0.331\nadjusted_satisfaction  0.0455447  0.1234767   0.369    0.713\navg_satisfaction       0.1497964  1.0164274   0.147    0.883\nstd_satisfaction      -0.0573383  1.0729419  -0.053    0.957\navg_income             0.0001944  0.0001290   1.507    0.135\nmedian_income         -0.0001814  0.0001491  -1.217    0.226\nincome_inequality     -0.0085178  0.0099008  -0.860    0.392\nGDP                    0.2758183  0.2600976   1.060    0.291\n\nResidual standard error: 0.4665 on 103 degrees of freedom\nMultiple R-squared:  0.8538,\tAdjusted R-squared:  0.8439 \nF-statistic: 85.96 on 7 and 103 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Интерпретация результатов\n\nВсе предикторы незначимы, при этом сама модель значима (p-value модели \\< 0.05). Это говорит о том, что в данных скорее всего есть мультиколлинеарность (про это чуть дальше).\n\nВопрос в том, что лучше, включить больше или меньше предикторов в модель?\n\n::: fragment\nОпасность переобучения (overfitting) -\\> нарисовать\n:::\n\n## Adjusted R-squared\n\nМодель с бОльшим числом предикторов почти всегда будет иметь больший $R^2$. Нужно ввести поправочный коэффициент, который штрафует модель за бОльшее число предикторов.\n\n$$\nR^2_{adj} = 1- (1-R^2)\\frac{n-1}{n-p}\n$$\n\np - количество параметров в модели, n - размер выборки.\n\nКроме этого, для отбора количества предикторов модели существует [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) (информационный критерий Акаике) и [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (байесовский информационный критерий).\n\n## Проблема мультиколлинеарности\n\nМультиколлинеарность --- наличие линейной зависимости между независимыми переменными в регрессионной модели.\n\nПри наличии мультиколлинеарности оценки параметров неточны, увеличиваются стандартные ошибки, а значит сложно интерпретировать влияние предикторов на отклик.\n\n## Variance inflation factor для оценки мультиколлинеарности\n\nДля предиктора $i$ считается линейная регрессия, где все остальные предикторы предсказывают предиктор $i$*.*\n\nСам VIF-фактор считается на основе полученного R^2^ регрессии:\n\n$$\nVIF = \\frac{1}{1 - R_i^2}\n$$\n\nЧем больше $R_i^2$, тем больше $VIF_i$. Это означает, что предиктор сам по себе хорошо объясняется другими предикторами. Какой VIF считать большим? Здесь нет единого мнения, но если он выше 3 (4, 5) и особенно если он выше 10, то с этим нужно что-то делать.\n\n## Подсчет VIF\n\nVIF можно посчитать с помощью функции `vif()` из пакета `car`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif(model_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nadjusted_satisfaction      avg_satisfaction      std_satisfaction \n          1178.079600            960.731610             58.304736 \n           avg_income         median_income     income_inequality \n           353.275696            350.629194              3.475757 \n                  GDP \n             5.136846 \n```\n:::\n:::\n\n\nЗдесь конечно экстремальная ситуация в связи с крайне высокой корреляцией переменных. Это видно и при подсчете матрицы корреляций, так что для реальной задачи не очень разумно включать сразу все переменные в модель, не проверив корреляции.\n\nКак вариант, можно заменить исходные предикторы новыми ортогональными переменными с помощью метода главных компонент (PCA).\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\ncorrplot::corrplot(cor(happy_score_data_numeric))\n```\n\n::: {.cell-output-display}\n![](statR_lecture10_files/figure-revealjs/unnamed-chunk-11-1.png){width=3000}\n:::\n:::\n\n\n## Отбор коррелирующих предикторов на основании VIF\n\nС помощью базовой функции `update()` можно убирать сильно коррелирущие предикторы.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_without_adj <- update(model_full, .~. -adjusted_satisfaction)\nvif(model_without_adj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n avg_satisfaction  std_satisfaction        avg_income     median_income \n         3.013224          1.498450        348.649164        345.961439 \nincome_inequality               GDP \n         3.447902          5.113941 \n```\n:::\n\n```{.r .cell-code}\nsummary(model_without_adj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ avg_satisfaction + std_satisfaction + \n    avg_income + median_income + income_inequality + GDP, data = happy_score_data_numeric)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22844 -0.28686  0.00393  0.29576  1.05390 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.0295472  0.5687129   5.327 5.81e-07 ***\navg_satisfaction   0.5241200  0.0566865   9.246 3.25e-15 ***\nstd_satisfaction  -0.4479771  0.1712908  -2.615   0.0102 *  \navg_income         0.0001890  0.0001276   1.480   0.1418    \nmedian_income     -0.0001751  0.0001475  -1.187   0.2379    \nincome_inequality -0.0081909  0.0098200  -0.834   0.4061    \nGDP                0.2822246  0.2584369   1.092   0.2773    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4646 on 104 degrees of freedom\nMultiple R-squared:  0.8537,\tAdjusted R-squared:  0.8452 \nF-statistic: 101.1 on 6 and 104 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nТеперь сильно коррелируют `avg_income` и `median_income`, что логично и было видно на корреляционном графике. Уберем одну из этих переменных.\n\n## Отбор коррелирующих предикторов на основании VIF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_without_avg_income <- update(model_without_adj, .~. -avg_income)\nvif(model_without_avg_income)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n avg_satisfaction  std_satisfaction     median_income income_inequality \n         3.000833          1.483673          4.111098          1.412751 \n              GDP \n         4.530807 \n```\n:::\n\n```{.r .cell-code}\nsummary(model_without_avg_income)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ avg_satisfaction + std_satisfaction + \n    median_income + income_inequality + GDP, data = happy_score_data_numeric)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22061 -0.27108  0.02655  0.30850  1.09323 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        2.605e+00  4.940e-01   5.274 7.21e-07 ***\navg_satisfaction   5.295e-01  5.689e-02   9.307 2.19e-15 ***\nstd_satisfaction  -4.732e-01  1.714e-01  -2.760  0.00681 ** \nmedian_income      4.193e-05  1.617e-05   2.594  0.01084 *  \nincome_inequality  2.978e-03  6.321e-03   0.471  0.63860    \nGDP                4.114e-01  2.446e-01   1.682  0.09559 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4672 on 105 degrees of freedom\nMultiple R-squared:  0.8506,\tAdjusted R-squared:  0.8435 \nF-statistic: 119.5 on 5 and 105 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Отбор моделей\n\nЕсть несколько основных стратегий отбора\n\n1.  Backward selection: full null -\\> null model\n2.  Forward selection: null model -\\> full model\n3.  Mixed selection: комбинация 1 и 2 способа\n\n## Уберем незначимые предикторы из модели: income_inequality\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_without_ineq <- update(model_without_avg_income, .~. -income_inequality)\nvif(model_without_ineq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\navg_satisfaction std_satisfaction    median_income              GDP \n        2.689961         1.479161         3.636432         4.479075 \n```\n:::\n\n```{.r .cell-code}\nsummary(model_without_ineq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ avg_satisfaction + std_satisfaction + \n    median_income + GDP, data = happy_score_data_numeric)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2107 -0.2632  0.0361  0.2976  1.0923 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       2.683e+00  4.641e-01   5.781 7.56e-08 ***\navg_satisfaction  5.381e-01  5.366e-02  10.028  < 2e-16 ***\nstd_satisfaction -4.687e-01  1.705e-01  -2.749  0.00703 ** \nmedian_income     3.935e-05  1.515e-05   2.597  0.01073 *  \nGDP               3.991e-01  2.423e-01   1.647  0.10255    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4655 on 106 degrees of freedom\nMultiple R-squared:  0.8503,\tAdjusted R-squared:  0.8446 \nF-statistic: 150.5 on 4 and 106 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Уберем незначимые предикторы из модели: GDP\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_without_gdp <-lm(happyScore ~ avg_satisfaction +\n                         std_satisfaction + median_income, \n                       data = happy_score_data_numeric)\nvif(model_without_gdp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\navg_satisfaction std_satisfaction    median_income \n        1.781739         1.303235         2.049282 \n```\n:::\n\n```{.r .cell-code}\nsummary(model_without_gdp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ avg_satisfaction + std_satisfaction + \n    median_income, data = happy_score_data_numeric)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.25075 -0.20623  0.01447  0.29152  1.12332 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       2.423e+00  4.399e-01   5.508 2.52e-07 ***\navg_satisfaction  5.895e-01  4.402e-02  13.390  < 2e-16 ***\nstd_satisfaction -3.719e-01  1.613e-01  -2.305   0.0231 *  \nmedian_income     5.583e-05  1.146e-05   4.871 3.87e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4692 on 107 degrees of freedom\nMultiple R-squared:  0.8464,\tAdjusted R-squared:  0.8421 \nF-statistic: 196.6 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Модель с дискретными предикторами\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_region <- lm(happyScore ~ avg_satisfaction +       median_income + region, data = happy_score_data)\nsummary(model_region)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = happyScore ~ avg_satisfaction + median_income + \n    region, data = happy_score_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4556 -0.2485  0.0000  0.2774  1.1139 \n\nCoefficients:\n                                          Estimate Std. Error t value Pr(>|t|)\n(Intercept)                              1.778e+00  6.883e-01   2.584   0.0112\navg_satisfaction                         4.997e-01  7.021e-02   7.117 1.76e-10\nmedian_income                            1.078e-04  2.351e-05   4.585 1.33e-05\nregion'Central and Eastern Europe'       2.236e-01  5.403e-01   0.414   0.6798\nregion'Eastern Asia'                    -1.113e-01  5.877e-01  -0.189   0.8501\nregion'Latin America and Caribbean'      5.088e-01  5.661e-01   0.899   0.3710\nregion'Middle East and Northern Africa'  1.936e-01  5.750e-01   0.337   0.7371\nregion'North America'                   -2.010e-01  5.818e-01  -0.346   0.7304\nregion'Southeastern Asia'                1.170e-01  5.850e-01   0.200   0.8419\nregion'Southern Asia'                    1.664e-02  6.040e-01   0.028   0.9781\nregion'Sub-Saharan Africa'               1.272e-01  5.801e-01   0.219   0.8269\nregion'Western Europe'                  -2.708e-01  4.871e-01  -0.556   0.5795\n                                           \n(Intercept)                             *  \navg_satisfaction                        ***\nmedian_income                           ***\nregion'Central and Eastern Europe'         \nregion'Eastern Asia'                       \nregion'Latin America and Caribbean'        \nregion'Middle East and Northern Africa'    \nregion'North America'                      \nregion'Southeastern Asia'                  \nregion'Southern Asia'                      \nregion'Sub-Saharan Africa'                 \nregion'Western Europe'                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4742 on 99 degrees of freedom\nMultiple R-squared:  0.8549,\tAdjusted R-squared:  0.8387 \nF-statistic: 53.02 on 11 and 99 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Спасибо за внимание!\n\nЕсли понравилось, переходите по [ссылке](https://www.tinkoff.ru/rm/ubogoeva.elena1/TSRBI31474):\n\n![](images/qrcode.png)\n",
    "supporting": [
      "statR_lecture10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}